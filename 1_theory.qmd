---
title: "Probability and Inference"
citation-location: document
---

Largely the probability and inference material that we will be examined on 
comes from [@casella2002statistical], though some of the later theory around 
point-estimation and hypothesis testing comes from [@lehmann2022testing; @lehmann1998theory]. Unfortunately, for much of the material towards the end (on 
the asymptotic properties of maximum likelihood estimates and the Wald, Score, and Likelihood Ratio Tests) the course materials refer heavily to notes from [Robert Gray](https://www.hsph.harvard.edu/profile/robert-gray/) ([google scholar](https://scholar.google.com/citations?user=QyY7JGIAAAAJ&hl=en&oi=sra)), and I don't believe his 
notes are publicly available, nor do I have permission to share them. As such, I'll 
try to find corroborating references for the same material when I get to that point. 

For the probability-focused material, we may also refer to a handful of other references
including [@blitzstein2019introduction; @bishop2006pattern; @gut2009intermediate; @stoyanov2013counterexamples; @Feller1968; @DasGupta2011; @Durrett2019]. 

--- 

At a high-level, how do we distinguish Probability and Statistics? 

In general, these are answers to dual problems:  Probability theory gives 
us the tools to carry out deductive reasoning, where based on _perfect, complete knowledge_ of 
random processes, we can derive true statements about the result (e.g., statements
about expectation, variance, etc.). On the other hand, statistical inference
allows us to go from _observed data_ and combine it with assumptions to 
draw conclusions inductively, which may or may not be correct. For example, a
statistical statement might be that a particular way of constructing a confidence interval 
(under assumptions) has a 95% probability of containing the true population parameter when 
repeatedly performing the same experiment (e.g., in the Frequentist paradigm). 

---

Outline of Probability: 

  - Introduction/Background
  - Probability Basics
  - Random Variables
  - Expected Values 
  - Families of Distributions 
  - Multiple Random Variables 
  - Independence and Conditional Expectation
  - Bivariate Dependence
  - Inequalities
  - Multivariate Normal
  - Statistics of Random Samples
  - Laws of Large Numbers and Central Limit Theorem 
  - Stochastic Processes

---

Outline of Inference: 

1. Introduction
    - Identifiability
    - Scale Families
    - Exponential Families
    - Moments
2. Principles of Data Reduction 
    - Data Reduction
    - Sufficiency 
        - Order Statistics
        - Factorization Theorem 
        - Minimal Sufficiency 
        - Ratio of Densities
    - Ancillary Statistics
        - Location-Scale Families
        - Conditioning / Conditionality Principle
    - Complete Families
        - Basu's Theorem 
    - Likelihood Principle
3. Methods of Estimation 
    - Method of Moments
    - Maximum Likelihood
        - Multiple Parameters
        - Invariance Property
        - Exponential Families
    - Other Methods
4. Evaluating Estimators
    - Mean Squared Error and Bias-Variance Tradeoff
    - Best Unbiased Estimation
        - Cram√©r-Rao Inequality
        - Rao-Blackwell 
        - Complete Families
5. Asymptotic Distributions 
    - Consistency 
    - Convergence in Distribution 
      - Slutsky's Theorem 
      - Delta Method 
      - Lyapunov Central Limit Theorem 
    - Multivariate Distributions 
      - Asymptotics
6. Intro to Hypothesis Testing 
    - Basic Concepts 
    - Likelihood Ratio Test
    - Type I and Type II Error Rate 
    - P-Values 
7. Theory of Optimal Tests 
    - UMP Tests 
      - Neyman-Pearson Lemma 
      - Randomized Tests 
      - More on Neyman-Pearson 
      - Karlin-Rubin 
      - Two-Sided Alternatives 
    - Locally Most Powerful Tests
    - Multiple Parameters
8. Distribution of MLEs and Asymptotic Efficiency 
    - Notation and Assumptions 
      - Multiple Parameters
    - Consistency of MLEs
    - Regularity Conditions
    - Asymptotic Distribution of MLEs
      - Non-IID Data
      - Multi-Parameter Models 
    - Asymptotic Relative Efficiency 
    - Asymptotic Efficiecny 
    - Asymptotic Confidence Intervals 
 9. Likelihood Ratio, Score, and Wald Tests
    - Likelihood Ratio, Score, and Wald Tests
    - Multidimensional Parameters
    - Composite Null 

--- 

Some particularly nice looking inference course materials are 
available online from Stanfords STATS 200: <https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/lectures.html>


## References