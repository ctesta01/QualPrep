[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Qualifying Exam Preparation",
    "section": "",
    "text": "Welcome to my notes from Summer 2024! These notes are principally for me to organize my thinking as I prepare for my PhD qualifying exams in Probability and Statistical Inference, (Statistical) Methods, and Data Structures & Algorithms. If they’re helpful to you, I’d love to hear about it! If you spot errors, please let me know. There is a commenting system enabled so that you can leave comments, and with support for LaTeX equations too!\nEnjoy!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome!</span>"
    ]
  },
  {
    "objectID": "1_theory/unit1_puzzles.html",
    "href": "1_theory/unit1_puzzles.html",
    "title": "Unit 1: History: Puzzles, Paradoxes, and Motivation",
    "section": "",
    "text": "History\nIt would be rather bland if I only spent all summer cramming, so I’ll try to capture the big picture as well.\nOne of the things I loved about the presentation in our Probability class (BST 230) was that we had a brief unit on the history of probability at the beginning.\nI trust that the reader can peruse the following references on their own:",
    "crumbs": [
      "Probability and Inference",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unit 1: History: Puzzles, Paradoxes, and Motivation</span>"
    ]
  },
  {
    "objectID": "theory.html",
    "href": "theory.html",
    "title": "Probability and Inference",
    "section": "",
    "text": "Largely the probability and inference material that we will be examined on comes from (Casella and Berger 2002), though some of the later theory around point-estimation and hypothesis testing comes from (E. L. Lehmann and Romano 2022; Erich L. Lehmann and Casella 1998). Unfortunately, for some of the material towards the end on the asymptotic properties of maximum likelihood estimates and the Wald, Score, and Likelihood Ratio Tests, the course materials refer heavily to notes from Robert Gray (google scholar), and I don’t believe his notes are publicly available, nor do I have permission to share them. As such, I’ll try to find corroborating references for the same material when I get to that point.\nFor the probability-focused material, we may also refer to a handful of other references including (Blitzstein and Hwang 2019; Bishop 2006; Gut 2009; Stoyanov 2013).\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. 2nd ed. Chapman; Hall/CRC.\n\n\nCasella, George, and Roger L Berger. 2002. Statistical Inference. Vol. 2. Duxbury Pacific Grove, CA.\n\n\nGut, Alan. 2009. An Intermediate Course in Probability. 2nd ed. Springer.\n\n\nLehmann, E. L., and Joseph P. Romano. 2022. Testing Statistical Hypotheses. Cham: Springer. https://doi.org/10.1007/978-3-030-70578-7.\n\n\nLehmann, Erich L., and George Casella. 1998. Theory of Point Estimation. 2nd ed. New York: Springer.\n\n\nStoyanov, Jordan M. 2013. Counterexamples in Probability. 3rd ed. Dover Publications.",
    "crumbs": [
      "Probability and Inference"
    ]
  },
  {
    "objectID": "2_methods.html",
    "href": "2_methods.html",
    "title": "Methods",
    "section": "",
    "text": "Lorem ipsum ad absurdum",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "1_theory.html",
    "href": "1_theory.html",
    "title": "Probability and Inference",
    "section": "",
    "text": "Largely the probability and inference material that we will be examined on comes from (Casella and Berger 2002), though some of the later theory around point-estimation and hypothesis testing comes from (E. L. Lehmann and Romano 2022; Erich L. Lehmann and Casella 1998). Unfortunately, for much of the material towards the end (on the asymptotic properties of maximum likelihood estimates and the Wald, Score, and Likelihood Ratio Tests) the course materials refer heavily to notes from Robert Gray (google scholar), and I don’t believe his notes are publicly available, nor do I have permission to share them. As such, I’ll try to find corroborating references for the same material when I get to that point.\n\nCasella, George, and Roger L Berger. 2002. Statistical Inference. Vol. 2. Duxbury Pacific Grove, CA.\n\nLehmann, E. L., and Joseph P. Romano. 2022. Testing Statistical Hypotheses. Cham: Springer. https://doi.org/10.1007/978-3-030-70578-7.\n\nLehmann, Erich L., and George Casella. 1998. Theory of Point Estimation. 2nd ed. New York: Springer.\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. 2nd ed. Chapman; Hall/CRC.\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\nGut, Alan. 2009. An Intermediate Course in Probability. 2nd ed. Springer.\n\nStoyanov, Jordan M. 2013. Counterexamples in Probability. 3rd ed. Dover Publications.\nFor the probability-focused material, we may also refer to a handful of other references including (Blitzstein and Hwang 2019; Bishop 2006; Gut 2009; Stoyanov 2013).\n\nAt a high-level, how do we distinguish Probability and Statistics?\nIn general, these are answers to dual problems: Probability theory gives us the tools to carry out deductive reasoning, where based on perfect, complete knowledge of random processes, we can derive true statements about the result (e.g., statements about expectation, variance, etc.). On the other hand, statistical inference allows us to go from observed data and combine it with assumptions to draw conclusions inductively, which may or may not be correct. For example, a statistical statement might be that a particular way of constructing a confidence interval (under assumptions) has a 95% probability of containing the true population parameter when repeatedly performing the same experiment (e.g., in the Frequentist paradigm).",
    "crumbs": [
      "Probability and Inference"
    ]
  },
  {
    "objectID": "3_datastructures.html",
    "href": "3_datastructures.html",
    "title": "Data Structures & Algorithms",
    "section": "",
    "text": "Lorem ipsum ad absurdum",
    "crumbs": [
      "Data Structures & Algorithms"
    ]
  },
  {
    "objectID": "1_theory/unit1_puzzles.html#monty-hall-problem",
    "href": "1_theory/unit1_puzzles.html#monty-hall-problem",
    "title": "Unit 1: History: Puzzles, Paradoxes, and Motivation",
    "section": "Monty Hall Problem",
    "text": "Monty Hall Problem\nThe Monty Hall problem can be stated so simply:\n\nSuppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host [Monty], who knows what’s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, “Do you want to pick door No. 2?” Is it to your advantage to switch your choice?\n\n\n\n\n\n\nDepiction of Monty Hall Problem: 3 doors, 3rd is open showing a goat\n\n\nHow should you ‘model’ your choices?\nDo we have all the information necessary?\nWe need to know: is there an equal initial probability of the car being behind each door? We’ll assume yes. Let’s additionally assume that Monty never reveals the car, and if he has a choice between two doors with goats behind them, he picks randomly either with probability \\(1/2\\).\nLet \\(M_i\\) denote the event where Monty opens the \\(i\\)th door. Also, let \\(D_i\\) denote the event where the car is behind door \\(i\\). We will assume as in the puzzle given that we have chosen door 1 and Monty has shown us there is a goat behind door 3.\nNotice that \\(P(D_1) = P(D_2) = P(D_3) = 1/3\\) by our first assumption. (These are unconditioned probabilities, before we learn anything). Our second assumption then implies the following table of probabilities for the joint events \\(P(M_i \\text{ and } D_j \\mid \\text{we chose door 1})\\) for \\(i, j \\in \\{ 1, 2, 3 \\}\\).\n\n\n\nJoint probabilities for where the car is and what door Monty shows\n\n\nBayes’ Rule Approach:\nNow we want to compare \\(P(D_1 | M_3)\\) vs. \\(P(D_2 | M_3)\\). We can use Bayes’ rule to calculate this using quantities we either already have or can get.\n\\[P(D_1 | M_3) = \\frac{P(M_3 | D_1)P(D_1)}{P(M_3)} = \\frac{(1/2) \\times (1/3)}{(1/2)} = \\frac{1}{3}.\\] \\[P(D_2 | M_3) = \\frac{P(M_3 | D_2)P(D_2)}{P(M_3)} = \\frac{1 \\times (1/3)}{(1/2)} = \\frac{2}{3}.\\]\nJustifications: We take by assumption that we chose door 1, so Monty will never choose door 1, and hence \\(P(M_3) = 1/2\\). In the first case, Monty’s hand is not forced. In the second case, Monty’s hand is forced, and hence \\(P(M_3 | D_2) = 1\\).\nThus we conclude that it is superior to switch to door 2.\n\n\n\n\n\n\nTip\n\n\n\nDerivation of Bayes’ Formula\nRecall the definition of conditional probability. \\[P(A | B) = P(A \\cap B) / P(B), \\quad \\quad P(B | A) = P(A \\cap B) / P(A)\\] \\[\\implies \\; P(A | B) P(B) = P(B | A) P(A)\\] \\[\\therefore P(B | A) = \\frac{P(A | B) P(B)}{P(A)}.\\]\n\n\nDirect Approach:\nYou could say, can’t we just use the definition of conditional probability straight away? Yes, you can. Essentially it’s constructing the above table that’s the important part in seeing the solution clearly.\n\\[P(D_1 | M_3 ) = \\frac{P(D_1 \\cap M_3)}{P(M_3)} = \\frac{1/6}{1/2} = \\frac{1}{3}.\\] \\[P(D_2 | M_3 ) = \\frac{P(D_2 \\cap M_3)}{P(M_3)} = \\frac{1/3}{1/2} = \\frac{2}{3}.\\]\nProblematic Approach:\nWhat’s wrong with saying \\(M_3\\) implies that either \\(D_1\\) or \\(D_2\\) must hold, so \\[P(D_1 | M_3) = \\frac{P(D_1 \\cap M_3)}{P(M_3)} = \\frac{P(D_1 \\cap \\{ D_1, D_2 \\})}{P(\\{D_1, D_2\\})} = \\frac{1/3}{2/3} = \\frac{1}{2},\\] and by the fact that \\(P(D_2 | M_3) = 1-P(D_1|M_3)\\), we have that \\(P(D_2 | M_3) = 1/2\\) as well, where \\(P(\\{D_1, D_2\\})\\) refers to the probability of either the car being behind door 1 or door 2.\nWell, \\(M_3\\) and \\(\\{D_1, D_2\\}\\) are not the same events. It is true that \\(M_3\\) implies the car is behind either door one or door two, but it’s not the case that the door being behind either door one or door two implies that Monty will open door 3. If they’re not the same events, then we cannot assume that their probabilities are equal. Hence, the above steps incorrectly replace \\(P(M_3)\\) with 2/3, and \\(P(D_1 \\cap M_3)\\), and in the numerator, the joint probability of \\(P(D_1 \\cap M_3)\\) need not equal \\(P(D_1 \\cap \\{ D_1, D_2 \\}) = P(D_1)\\).",
    "crumbs": [
      "Probability and Inference",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unit 1: History: Puzzles, Paradoxes, and Motivation</span>"
    ]
  }
]