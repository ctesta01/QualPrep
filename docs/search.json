[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Qualifying Exam Preparation",
    "section": "",
    "text": "Welcome to my notes from Summer 2024! These notes are principally for me to organize my thinking as I prepare for my PhD qualifying exams in Probability and Statistical Inference, (Statistical) Methods, and Data Structures & Algorithms. If they’re helpful to you, I’d love to hear about it! If you spot errors, please let me know. There is a commenting system enabled so that you can leave comments, and with support for LaTeX equations too!\nEnjoy!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome!</span>"
    ]
  },
  {
    "objectID": "1_theory/unit1_puzzles.html",
    "href": "1_theory/unit1_puzzles.html",
    "title": "Unit 1: History: Puzzles, Paradoxes, and Motivation",
    "section": "",
    "text": "History\nOne of the things I loved about the presentation in our Probability class (BST 230) was that we had a brief unit on the history of probability at the beginning.\nI trust that the reader can peruse the following references on their own:",
    "crumbs": [
      "Probability and Inference",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unit 1: History: Puzzles, Paradoxes, and Motivation</span>"
    ]
  },
  {
    "objectID": "theory.html",
    "href": "theory.html",
    "title": "Probability and Inference",
    "section": "",
    "text": "Largely the probability and inference material that we will be examined on comes from (Casella and Berger 2002), though some of the later theory around point-estimation and hypothesis testing comes from (E. L. Lehmann and Romano 2022; Erich L. Lehmann and Casella 1998). Unfortunately, for some of the material towards the end on the asymptotic properties of maximum likelihood estimates and the Wald, Score, and Likelihood Ratio Tests, the course materials refer heavily to notes from Robert Gray (google scholar), and I don’t believe his notes are publicly available, nor do I have permission to share them. As such, I’ll try to find corroborating references for the same material when I get to that point.\nFor the probability-focused material, we may also refer to a handful of other references including (Blitzstein and Hwang 2019; Bishop 2006; Gut 2009; Stoyanov 2013).\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. 2nd ed. Chapman; Hall/CRC.\n\n\nCasella, George, and Roger L Berger. 2002. Statistical Inference. Vol. 2. Duxbury Pacific Grove, CA.\n\n\nGut, Alan. 2009. An Intermediate Course in Probability. 2nd ed. Springer.\n\n\nLehmann, E. L., and Joseph P. Romano. 2022. Testing Statistical Hypotheses. Cham: Springer. https://doi.org/10.1007/978-3-030-70578-7.\n\n\nLehmann, Erich L., and George Casella. 1998. Theory of Point Estimation. 2nd ed. New York: Springer.\n\n\nStoyanov, Jordan M. 2013. Counterexamples in Probability. 3rd ed. Dover Publications.",
    "crumbs": [
      "Probability and Inference"
    ]
  },
  {
    "objectID": "2_methods.html",
    "href": "2_methods.html",
    "title": "Methods",
    "section": "",
    "text": "BST 232, Methods, has the following description:\n\n[BST 232 is an] introductory course in the analysis of Gaussian and categorical data, the general linear regression model, ANOVA, robust alternatives based on permutations, model building, resampling methods (bootstrap and jackknife), contingency tables, exact methods, logistic regression.\n\nCourse Outline:\n\nLinear Regression\nDiagnostics\nModel Selection\nBootstrap Permutation\nHeteroscedastic Errors\nProportions and Contingency Tables\nLikelihood Theory\nGLMs\nBinary Outcomes\nCount Outcomes\nSurvival Outcomes\n\nThe listed primary references are:\n\nKutner et al. (2005)\nAgresti (2013)\nCollett (2015)\n\nSecondary references:\n\nVittinghoff et al. (2012)\nChatterjee and Hadi (2006)\nMcCullagh and Nelder (1989)\nFaraway (2016)\nJames et al. (2013)\n\nSome online references:\n\nFrank Harrell’s Regression Modeling Strategies https://hbiostat.org/rmsc/\nAdvanced Statistical Modeling III from Durham University https://bookdown.org/cnguyen/ASM_term2_lecture_notes/\nRegression Modelling for Biostatistics 1 from Biostatistics Collaboration of Australia https://bookdown.org/stephane_heritier/RM1TEST/\n\nReferences:\n\n\n\n\n\nAgresti, Alan. 2013. Categorical Data Analysis. 3rd ed. Hoboken, NJ: Wiley.\n\n\nChatterjee, Samprit, and Ali S. Hadi. 2006. Regression Analysis by Example. 4th ed. John Wiley & Sons.\n\n\nCollett, David. 2015. Modelling Survival Data in Medical Research. 3rd ed. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://www.crcpress.com/Modelling-Survival-Data-in-Medical-Research/Collett/p/book/9781439856789.\n\n\nFaraway, Julian J. 2016. Extending the Linear Model with r: Generalized Linear, Mixed Effects and Nonparametric Regression Models. 2nd ed. Boca Raton: CRC Press.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning: With Applications in r. Springer. https://faculty.marshall.usc.edu/gareth-james/ISL/.\n\n\nKutner, Michael H., Christopher J. Nachtsheim, John Neter, and William Li. 2005. Applied Linear Statistical Models. 5th ed. Boston: McGraw-Hill Irwin.\n\n\nMcCullagh, P., and J. A. Nelder. 1989. Generalized Linear Models. 2nd ed. London: Chapman; Hall.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. New York: Springer.",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "1_theory.html",
    "href": "1_theory.html",
    "title": "Probability and Inference",
    "section": "",
    "text": "References",
    "crumbs": [
      "Probability and Inference"
    ]
  },
  {
    "objectID": "3_datastructures.html",
    "href": "3_datastructures.html",
    "title": "Data Structures & Algorithms",
    "section": "",
    "text": "Course outline:\n\nRandom Numbers\nConcepts of Algorithms\nData Structures\nHeaps\nGreedy Algorithms and Dynamic Programming\nParallel Computing\nP and NP\nNumerical Aspects of Algorithms\nSystems of Linear Equations\nLeast Squares\nEigenvalues and Sparse Matrices\nSystems of Nonlinear Equations\nNumerical Integration and MCMC\nNumerical Optimization\n\nSome online resources I’ve found helpful:\n\nSteven Skiena https://m.youtube.com/watch?v=A2bFN3MyNDA&list=PLOtl7M3yp-DX32N0fVIyvn7ipWKNGmwpp\nMIT 6.006 https://m.youtube.com/playlist?list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb\nAn Open Guide to Data Structures and Algorithms https://pressbooks.palni.org/anopenguidetodatastructuresandalgorithms/\nAlgorithms by Jeff Erickson http://jeffe.cs.illinois.edu/teaching/algorithms/#book\nAwesome Algorithms on GitHub https://github.com/tayllan/awesome-algorithms",
    "crumbs": [
      "Data Structures & Algorithms"
    ]
  },
  {
    "objectID": "1_theory/unit1_puzzles.html#monty-hall-problem",
    "href": "1_theory/unit1_puzzles.html#monty-hall-problem",
    "title": "Unit 1: History: Puzzles, Paradoxes, and Motivation",
    "section": "Monty Hall Problem",
    "text": "Monty Hall Problem\nThe Monty Hall problem can be stated so simply:\n\nSuppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host [Monty], who knows what’s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, “Do you want to pick door No. 2?” Is it to your advantage to switch your choice?\n\n\n\n\n\n\nDepiction of Monty Hall Problem: 3 doors, 3rd is open showing a goat\n\n\nHow should you ‘model’ your choices?\nDo we have all the information necessary?\nWe need to know: is there an equal initial probability of the car being behind each door? We’ll assume yes. Let’s additionally assume that Monty never reveals the car, and if he has a choice between two doors with goats behind them, he picks randomly either with probability \\(1/2\\).\nLet \\(M_i\\) denote the event where Monty opens the \\(i\\)th door. Also, let \\(D_i\\) denote the event where the car is behind door \\(i\\). We will assume as in the puzzle given that we have chosen door 1 and Monty has shown us there is a goat behind door 3.\nNotice that \\(P(D_1) = P(D_2) = P(D_3) = 1/3\\) by our first assumption. (These are unconditioned probabilities, before we learn anything). Our second assumption then implies the following table of probabilities for the joint events \\(P(M_i \\text{ and } D_j \\mid \\text{we chose door 1})\\) for \\(i, j \\in \\{ 1, 2, 3 \\}\\).\n\n\n\nJoint probabilities for where the car is and what door Monty shows\n\n\nBayes’ Rule Approach:\nNow we want to compare \\(P(D_1 | M_3)\\) vs. \\(P(D_2 | M_3)\\). We can use Bayes’ rule to calculate this using quantities we either already have or can get.\n\\[P(D_1 | M_3) = \\frac{P(M_3 | D_1)P(D_1)}{P(M_3)} = \\frac{(1/2) \\times (1/3)}{(1/2)} = \\frac{1}{3}.\\] \\[P(D_2 | M_3) = \\frac{P(M_3 | D_2)P(D_2)}{P(M_3)} = \\frac{1 \\times (1/3)}{(1/2)} = \\frac{2}{3}.\\]\nJustifications: We take by assumption that we chose door 1, so Monty will never choose door 1, and hence \\(P(M_3) = 1/2\\). In the first case, Monty’s hand is not forced. In the second case, Monty’s hand is forced, and hence \\(P(M_3 | D_2) = 1\\).\nThus we conclude that it is superior to switch to door 2.\n\n\n\n\n\n\nTip\n\n\n\nDerivation of Bayes’ Formula\nRecall the definition of conditional probability. \\[P(A | B) = P(A \\cap B) / P(B), \\quad \\quad P(B | A) = P(A \\cap B) / P(A)\\] \\[\\implies \\; P(A | B) P(B) = P(B | A) P(A)\\] \\[\\therefore P(B | A) = \\frac{P(A | B) P(B)}{P(A)}.\\]\n\n\nDirect Approach:\nYou could say, can’t we just use the definition of conditional probability straight away? Yes, you can. Essentially it’s constructing the above table that’s the important part in seeing the solution clearly.\n\\[P(D_1 | M_3 ) = \\frac{P(D_1 \\cap M_3)}{P(M_3)} = \\frac{1/6}{1/2} = \\frac{1}{3}.\\] \\[P(D_2 | M_3 ) = \\frac{P(D_2 \\cap M_3)}{P(M_3)} = \\frac{1/3}{1/2} = \\frac{2}{3}.\\]\nProblematic Approach:\nWhat’s wrong with saying \\(M_3\\) implies that either \\(D_1\\) or \\(D_2\\) must hold, so \\[P(D_1 | M_3) = \\frac{P(D_1 \\cap M_3)}{P(M_3)} = \\frac{P(D_1 \\cap \\{ D_1, D_2 \\})}{P(\\{D_1, D_2\\})} = \\frac{1/3}{2/3} = \\frac{1}{2},\\] and by the fact that \\(P(D_2 | M_3) = 1-P(D_1|M_3)\\), we have that \\(P(D_2 | M_3) = 1/2\\) as well, where \\(P(\\{D_1, D_2\\})\\) refers to the probability of either the car being behind door 1 or door 2.\nWell, \\(M_3\\) and \\(\\{D_1, D_2\\}\\) are not the same events. It is true that \\(M_3\\) implies the car is behind either door one or door two, but it’s not the case that the door being behind either door one or door two implies that Monty will open door 3. If they’re not the same events, then we cannot assume that their probabilities are equal. Hence, the above steps incorrectly replace \\(P(M_3)\\) with 2/3, and \\(P(D_1 \\cap M_3)\\), and in the numerator, the joint probability of \\(P(D_1 \\cap M_3)\\) need not equal \\(P(D_1 \\cap \\{ D_1, D_2 \\}) = P(D_1)\\).",
    "crumbs": [
      "Probability and Inference",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unit 1: History: Puzzles, Paradoxes, and Motivation</span>"
    ]
  },
  {
    "objectID": "1_theory/unit1_puzzles.html#puzzle-1",
    "href": "1_theory/unit1_puzzles.html#puzzle-1",
    "title": "Unit 1: History: Puzzles, Paradoxes, and Motivation",
    "section": "Puzzle #1",
    "text": "Puzzle #1\n\nThere are 43 cookies to be given out at random to 10 children. What is the probability that each child gets at least 10 cookies?\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\nExercise in flawed thinking.\n\nFind the flaw in the following logic:\nConsider the outcomes where each child has 2+ cookies. Then, for 20 cookies, we already know how those are distributed (each of the 10 children gets two cookies). The remaining 23 cookies can be assigned arbitrarily, so across the 10 kids there are a remaining \\(10^{23}\\) ways to distribute the cookies for each of the \\({43 \\choose 20}\\) ways that the 20-cookies already spoken-for can be distributed.\nThat gives us \\[\\frac{P(\\text{ways to get to every child having 2+ cookies})}{P(\\text{total number of ways to distribute 43 cookies})} =\n\\frac{{43 \\choose 20} \\cdot 10^{23}}{10^{43}} = \\frac{{43 \\choose 20}}{10^{20}}.\\]\n\n\nClick to reveal the problem in the above logic.\n\nI think we’re double-counting solutions when we say that for each of the \\({43 \\choose 20}\\) ways that we can say that the 20 cookies are already spoken for, there are another \\(10^{23}\\) ways to assign the other cookies.\nLet’s take a smaller example: assigning 4 cookies to 2 students, and I want to know how often to expect each student gets 1+ cookie. If I consider outcomes where cookies 1, 2, 3, 4 get assigned to children \\(1, 2, *, *\\), I would count \\(1,2,1,2\\) towards this tally. But if I later also consider outcomes of the form \\(*, *, 1, 2\\), I would again count the outcome \\(1,2,1,2\\) twice. This is wrong, and a crucial mistake.\nIn general, the real flaw was in not adequately formalizing our work and using heuristics like “if cookies are spoken for,” which is not a rigorously defined probability concept.\n\n\n\n\nFirst, notice that this is a classic multinomial setup. The multinomial distribution can be thought of as giving the probability of observing the outcome \\(i \\in \\{ 1, ..., k \\}\\) coming up \\(x_i\\) times when rolling a \\(k\\)-sided die \\(n\\) times, and is a generalization of the Binomial distribution. We have that the probability of each outcome \\(i\\) coming up on a single roll is given by \\(\\pi_i\\).\nIf the so-called die is fair, \\(\\pi_i = \\pi_{i'}\\; \\forall i, i' \\in \\{ 1, ..., k\\}\\) or the die is unfair and \\(\\pi_i\\) is not necessarily equal to \\(\\pi_i'\\). In all cases, we assume \\(\\sum \\pi_i = 1\\).\nLet \\(X = (X_1, ..., X_k)\\) be a \\(\\text{Multinomial}(n, k, \\pi)\\) distributed where \\(\\pi = (\\pi_1, ..., \\pi_k)\\).\nThe density of the Multinomial distribution is\n\\[\n\\begin{aligned}\nP(X = x) & = {n \\choose x_1,...,x_k!} \\prod_{i=1}^k \\pi_i^{x_i} \\\\\n& = \\frac{n!}{x_1!\\cdots x_k!}\\pi_1^{x_1} \\cdots \\pi_k^{x_k}.\n\\end{aligned}\n\\]\n\n\n\nIntuition\n\nI think of it this way: in the traditional Binomial outcome, there’s two outcomes that are being tallied, 1 or 0, and either 1 happens with probability \\(\\pi_1\\), or 0 happens with probability \\(1-\\pi_1\\) in each trial. Here, the \\(\\sum \\pi_i = 1\\) assumption is making it so that either outcome \\(i\\) happens with probability \\(\\pi_i\\), or one of the other \\(i' \\in \\{ 1, ..., k\\} \\backslash i\\) outcomes happens with probability \\(1 - \\pi_i\\).\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe Multinomial Theorem\n\nFor any positive integer \\(m\\) and non-negative integer \\(n\\), \\[(x_1 + ... + x_m)^n = \\sum_{k_1+...+ k_m = n} {n \\choose k_1, ..., k_m} \\prod_{i=1}^m x_i^{k_i},\\] where \\(k_1, ..., k_m \\in \\mathbb N\\) and \\({n \\choose k_1, ..., k_m} = \\frac{n!}{k_1!\\cdots k_m!}\\).\n\n\n\nReturning to the problem, let \\(p_n\\) denote the probability that all 10 children receive at least 2 cookies each given that \\(n\\) are given out uniformly at random.\nLet \\(X_1, ..., X_{10}\\) denote the random variables representing how many cookies each of the 10 children gets. Notice that \\[\n{\\small\nX_1, ..., X_{10} \\Bigg\\vert \\sum_{i=1}^{10} X_i = 43 \\sim\n\\text{Multinomial}(n = 43, k = 10, \\pi_i = 1/10),}\n\\] where \\(\\pi_i = 1/10\\) indicates that the distribution of cookies is uniformly random (e.g., equal probability) across the 10 children.\n\n\n\n\n\n\nPoisson-Multinomial Relationship\nThe Poisson and Multinomial distributions have an interesting relationship. When the outcomes \\(X_1, ..., X_k\\) are such that \\(X_i \\sim \\text{Poisson}(\\lambda_i)\\), then \\(\\sum_{i=1}^k X_i \\sim \\text{Poisson}(\\sum_{i=1}^k \\lambda_i).\\)\nOne can easily derive via the definition of conditional probability that \\[(X_1, ..., X_k) {\\Bigg\\vert} \\sum_{i=1}^k X_i = N = n \\sim \\text{Multinomial}(n, k, \\pi),\\] where \\(\\pi = \\left(\\frac{\\lambda_1}{\\sum \\lambda_i}, ..., \\frac{\\lambda_k}{\\sum \\lambda_i}\\right)\\).\nSee here for example.\n\n\\[\n\\begin{aligned}\nP(X = x \\Big \\vert N = n) & = \\frac{P(X = x, N = n)}{P(N = n)}  \\\\\n& = \\frac{P(X = x)}{P(N = n)} \\\\\n& = \\left( \\frac{e^{-\\sum \\lambda_i} \\prod \\lambda_i^{x_i}}{\\prod x_i!} \\right) \\Bigg / \\left( \\frac{e^{-\\sum \\lambda_i} (\\sum \\lambda_i)^{n}}{n!} \\right) \\\\\n& = {n \\choose x_1, ..., x_k} \\frac{\\prod \\lambda_i^{x_i}}{\\left( \\sum \\lambda_i \\right)^n} \\\\\n& = {n \\choose x_1, ..., x_k} \\left( \\frac{\\lambda_i}{\\sum \\lambda_i} \\right)^{x_i} \\\\\n& \\sim \\text{Multinomial}(n, k, \\pi).\n\\end{aligned}\n\\]\n\n\n\n\nSince no information was given to us about how it came about that \\(n = 43\\) cookies were given out, let’s assume that it was the result of a \\(\\text{Poisson}(\\lambda)\\) process. This implies that the \\(X_i\\) are independently and identically distributed as \\(\\text{Poisson}(\\lambda/10)\\) by a similar argument in the reverse direction.\n\n\\[P(X = x \\Bigg| \\sum X_i = n) = \\frac{P(X = x, \\sum X_i = n)}{\\underbrace{P(\\sum X_i = n)}_{\\text{Poisson}(\\lambda)}},\\] and \\(X = x \\implies \\sum X_i = \\sum x_i = n\\), so \\(P(X = x, \\sum X_i = n) = P(X = x)\\) as long as \\(n = \\sum x_i\\). \\[\n\\begin{aligned}\n\\therefore \\;\\; P(X = x) & = P(X = x \\Bigg| \\sum X_i = n) \\cdot P(\\sum X_i = n) \\\\\n& = \\left[ {n \\choose \\pi_1, ..., \\pi_k} \\prod \\pi_i^{x_i} \\right] \\cdot \\left( \\frac{e^{-\\lambda} \\lambda^{\\sum x_i}}{\\sum x_i! } \\right) \\\\\n& = \\frac{\\cancel{n!}}{x_1!\\cdots x_k!} \\pi_1^{x_1}\\cdots \\pi_k^{x_k} \\left( \\frac{e^{-\\lambda} \\lambda^{n}}{\\cancel{n!}} \\right).\n\\end{aligned}\n\\] Assume as given in the problem that the cookies are uniformly randomly given out, and \\(\\pi_i = \\pi_{i'}\\;  \\forall i, i' \\in \\{ 1, ..., k \\}\\); this single probability must be \\(1/k\\) (or in our case, \\(k = 10\\) children).\n\\[\n\\begin{aligned}\nP(X = x) & = \\frac{e^{-\\lambda}}{x_1! \\cdots x_k!} \\left( \\frac{\\lambda}{k} \\right)^{\\sum x_i} \\\\\n& = \\prod_{i=1}^k \\frac{e^{-\\lambda/k} \\left( \\frac{\\lambda}{k} \\right)^{x_i}}{x_i!}, \\\\\n\\text{a product of the }&\\text{density for $k$ iid Poisson}\\left(\\frac{\\lambda}{k}\\right) \\text{ variables}.\n\\end{aligned}\n\\]\n\nWe said that if \\(n\\) cookies are given out, then there’s a \\(p_n\\) probability that the 10 children all receive 2+ cookies. Then without conditioning on the number of cookies given out, the probability that all kids receive 2+ cookies is given by \\[\\E\\left[\\E[p_n | N = n]\\right] = \\sum_{n=0}^\\infty \\frac{e^{-\\lambda} \\lambda^n}{n!} p_n.\\]\nOn the other hand, since the \\(X_1, ..., X_k\\) are iid Poisson, the probability that all 10 kids receive 2+ cookies is \\(P(X_1 \\geq 2)^{10} = (1-P(X_1 \\leq 1))^{10} =\n(1-e^{-\\frac{\\lambda}{10}} - \\frac{\\lambda}{10}e^{-\\frac{\\lambda}{10}})^{10}\\).\nNow we have that \\[\n\\sum_{n=0}^\\infty \\frac{e^{-\\lambda} \\lambda^n}{n!} p_n\n=\n\\left[1-e^{-\\frac{\\lambda}{10}} - \\frac{\\lambda}{10}e^{-\\frac{\\lambda}{10}}\\right]^{10}.\n\\]\n\n\nReference materials on Poissonization and related ideas.\n\nSo, I didn’t know the first thing about “Poissonization,” but I’ve been trying to learn.\nIt appears that Mary Wootters [site] and C. Seshadhri [site] have nice online YouTube playlists of their randomized algorithms lectures in which they talked about Poissonization.\nSee the specific videos:\n\nhttps://www.youtube.com/watch?v=P71s05H2T5E\nhttps://www.youtube.com/watch?v=4lvfTKiLyXw\n\nI get the sense that the Chernoff bound for the Poisson-tail is something I should learn about further. The application of Stirling’s approximation to the distribution of deviations from the expected value of a Poisson process that Seshadhri presents is quite beautiful, and something I’d like to learn more about.\nThere’s also this interesting approximation of the Multinomial cell-specific count outcomes as Poisson that I saw popping up here and there (Ahmad 1985).\n\n\nAhmad, I. A. 1985. “On the Poisson Approximation of Multinomial Probabilities.” Statistics & Probability Letters 3 (1): 55–56. https://doi.org/10.1016/0167-7152(85)90013-6.\nNow, recall that \\(e^{x}\\) has a series expansion. If we multiply both sides by \\(e^{\\lambda}\\) and by \\(43!\\), we should find that the coefficient on the left-hand-side series for \\(\\lambda^{43}\\) is just \\(p_n\\), and the right-hand-side series coefficient for \\(\\lambda^{43}\\) gives an expression we can evaluate for \\(p_{43}\\). They must be equal, because in order for two convergent power series to coincide on a non-empty interval, their coefficients must be equal.\nSo what we’re going to do is expand the function \\(e^{x} = \\sum_{t=0}^\\infty \\frac{x^t}{t!}\\) wherever we see it on the modified right-hand-side and use that to identify the coefficient of \\(\\lambda^{43}\\), which is \\(p_{43}\\).\nHowever, this is a bit hard to do by hand, so we’ll use the symbolic algebra library sympy in Python to do it for us.\nfrom sympy import symbols, exp, factorial, series, Pow\n\nlambda_ = symbols('lambda')\n\ninner_expression = 1 - exp(-lambda_/10) - (lambda_/10)*exp(-lambda_/10)\n\nraised_expression = inner_expression**10 \n\ncomplete_expression = exp(lambda_) * raised_expression\n\nexpanded_series = series(complete_expression, x=lambda_, n=44).removeO()\n\ncoeff_lambda_43 = expanded_series.coeff(lambda_**43)\n\np_43 = factorial(43) * coeff_lambda_43\np_43\n\\[\n\\frac{38360235213946776318553037176114920309}{78125000000000000000000000000000000000} \\approx 0.491\n\\]\nThus we conclude that if 43 cookies are given out to 10 children uniformly at random, then the probability that each child receives at least 2 cookies is \\(\\approx .491.\\)\nLet’s see if we can confirm that via a simple simulation in R:\nset.seed(1234)\n\nnum_trials &lt;- 100000  # Number of simulations\nnum_children &lt;- 10    # Number of children\nnum_cookies &lt;- 43     # Number of cookies\n\nresults &lt;- replicate(num_trials, {\n  cookies &lt;- sample(1:num_children, num_cookies, replace = TRUE)\n  counts &lt;- table(factor(cookies, levels = 1:10))\n  all(counts &gt;= 2)\n})\n\nprob_estimate &lt;- mean(results)\nvar_estimate &lt;- var(results) / num_trials\n\nprob_estimate\nvar_estimate\n&gt; prob_estimate\n[1] 0.49178\n&gt; var_estimate\n[1] 2.499349e-06\nAlright, I think I’m happy with that.",
    "crumbs": [
      "Probability and Inference",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unit 1: History: Puzzles, Paradoxes, and Motivation</span>"
    ]
  },
  {
    "objectID": "1_theory.html#references",
    "href": "1_theory.html#references",
    "title": "Probability and Inference",
    "section": "",
    "text": "Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. 2nd ed. Chapman; Hall/CRC.\n\n\nCasella, George, and Roger L Berger. 2002. Statistical Inference. Vol. 2. Duxbury Pacific Grove, CA.\n\n\nDasGupta, Anirban. 2011. Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics. New York: Springer. https://doi.org/10.1007/978-1-4419-9634-3.\n\n\nDurrett, Rick. 2019. Probability: Theory and Examples. 5th ed. Cambridge Series in Statistical and Probabilistic Mathematics 49. Cambridge University Press. https://www.cambridge.org/core/books/probability-theory-and-examples/82A04569DDBEE06644D01E73E00E10EE.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications, Volume 1. 3rd ed. John Wiley & Sons.\n\n\nGut, Alan. 2009. An Intermediate Course in Probability. 2nd ed. Springer.\n\n\nLehmann, E. L., and Joseph P. Romano. 2022. Testing Statistical Hypotheses. Cham: Springer. https://doi.org/10.1007/978-3-030-70578-7.\n\n\nLehmann, Erich L., and George Casella. 1998. Theory of Point Estimation. 2nd ed. New York: Springer.\n\n\nStoyanov, Jordan M. 2013. Counterexamples in Probability. 3rd ed. Dover Publications.",
    "crumbs": [
      "Probability and Inference"
    ]
  },
  {
    "objectID": "4_appendices.html",
    "href": "4_appendices.html",
    "title": "Appendices on Supplemental Material",
    "section": "",
    "text": "Material that doesn’t quite fit anywhere else, including:\n\nReminders of Important Calculus Concepts\nRefreshers on Important Real Analysis Concepts\nRefreshers on Linear Algebra Concepts",
    "crumbs": [
      "Appendices on Supplemental Material"
    ]
  },
  {
    "objectID": "4_appendix.html",
    "href": "4_appendix.html",
    "title": "Appendices on Supplemental Material",
    "section": "",
    "text": "Material that doesn’t quite fit anywhere else, including:\n\nReminders of Important Calculus Concepts\nRefreshers on Important Real Analysis Concepts\nRefreshers on Linear Algebra Concepts",
    "crumbs": [
      "Appendices on Supplemental Material"
    ]
  },
  {
    "objectID": "1_theory/01_background_puzzles.html",
    "href": "1_theory/01_background_puzzles.html",
    "title": "History: Puzzles, Paradoxes, and Motivation",
    "section": "",
    "text": "History\nOne of the things I loved about the presentation in our Probability class (BST 230) was that we had a brief unit on the history of probability at the beginning.\nI trust that the reader can peruse the following references on their own:",
    "crumbs": [
      "Probability and Inference",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History: Puzzles, Paradoxes, and Motivation</span>"
    ]
  },
  {
    "objectID": "1_theory/01_background_puzzles.html#monty-hall-problem",
    "href": "1_theory/01_background_puzzles.html#monty-hall-problem",
    "title": "History: Puzzles, Paradoxes, and Motivation",
    "section": "Monty Hall Problem",
    "text": "Monty Hall Problem\nThe Monty Hall problem can be stated so simply:\n\nSuppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host [Monty], who knows what’s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, “Do you want to pick door No. 2?” Is it to your advantage to switch your choice?\n\n\n\n\n\n\nDepiction of Monty Hall Problem: 3 doors, 3rd is open showing a goat\n\n\nHow should you ‘model’ your choices?\nDo we have all the information necessary?\nWe need to know: is there an equal initial probability of the car being behind each door? We’ll assume yes. Let’s additionally assume that Monty never reveals the car, and if he has a choice between two doors with goats behind them, he picks randomly either with probability \\(1/2\\).\nLet \\(M_i\\) denote the event where Monty opens the \\(i\\)th door. Also, let \\(D_i\\) denote the event where the car is behind door \\(i\\). We will assume as in the puzzle given that we have chosen door 1 and Monty has shown us there is a goat behind door 3.\nNotice that \\(P(D_1) = P(D_2) = P(D_3) = 1/3\\) by our first assumption. (These are unconditioned probabilities, before we learn anything). Our second assumption then implies the following table of probabilities for the joint events \\(P(M_i \\text{ and } D_j \\mid \\text{we chose door 1})\\) for \\(i, j \\in \\{ 1, 2, 3 \\}\\).\n\n\n\nJoint probabilities for where the car is and what door Monty shows\n\n\nBayes’ Rule Approach:\nNow we want to compare \\(P(D_1 | M_3)\\) vs. \\(P(D_2 | M_3)\\). We can use Bayes’ rule to calculate this using quantities we either already have or can get.\n\\[P(D_1 | M_3) = \\frac{P(M_3 | D_1)P(D_1)}{P(M_3)} = \\frac{(1/2) \\times (1/3)}{(1/2)} = \\frac{1}{3}.\\] \\[P(D_2 | M_3) = \\frac{P(M_3 | D_2)P(D_2)}{P(M_3)} = \\frac{1 \\times (1/3)}{(1/2)} = \\frac{2}{3}.\\]\nJustifications: We take by assumption that we chose door 1, so Monty will never choose door 1, and hence \\(P(M_3) = 1/2\\). In the first case, Monty’s hand is not forced. In the second case, Monty’s hand is forced, and hence \\(P(M_3 | D_2) = 1\\).\nThus we conclude that it is superior to switch to door 2.\n\n\n\n\n\n\nTip\n\n\n\nDerivation of Bayes’ Formula\nRecall the definition of conditional probability. \\[P(A | B) = P(A \\cap B) / P(B), \\quad \\quad P(B | A) = P(A \\cap B) / P(A)\\] \\[\\implies \\; P(A | B) P(B) = P(B | A) P(A)\\] \\[\\therefore P(B | A) = \\frac{P(A | B) P(B)}{P(A)}.\\]\n\n\nDirect Approach:\nYou could say, can’t we just use the definition of conditional probability straight away? Yes, you can. Essentially it’s constructing the above table that’s the important part in seeing the solution clearly.\n\\[P(D_1 | M_3 ) = \\frac{P(D_1 \\cap M_3)}{P(M_3)} = \\frac{1/6}{1/2} = \\frac{1}{3}.\\] \\[P(D_2 | M_3 ) = \\frac{P(D_2 \\cap M_3)}{P(M_3)} = \\frac{1/3}{1/2} = \\frac{2}{3}.\\]\nProblematic Approach:\nWhat’s wrong with saying \\(M_3\\) implies that either \\(D_1\\) or \\(D_2\\) must hold, so \\[P(D_1 | M_3) = \\frac{P(D_1 \\cap M_3)}{P(M_3)} = \\frac{P(D_1 \\cap \\{ D_1, D_2 \\})}{P(\\{D_1, D_2\\})} = \\frac{1/3}{2/3} = \\frac{1}{2},\\] and by the fact that \\(P(D_2 | M_3) = 1-P(D_1|M_3)\\), we have that \\(P(D_2 | M_3) = 1/2\\) as well, where \\(P(\\{D_1, D_2\\})\\) refers to the probability of either the car being behind door 1 or door 2.\nWell, \\(M_3\\) and \\(\\{D_1, D_2\\}\\) are not the same events. It is true that \\(M_3\\) implies the car is behind either door one or door two, but it’s not the case that the door being behind either door one or door two implies that Monty will open door 3. If they’re not the same events, then we cannot assume that their probabilities are equal. Hence, the above steps incorrectly replace \\(P(M_3)\\) with 2/3, and \\(P(D_1 \\cap M_3)\\), and in the numerator, the joint probability of \\(P(D_1 \\cap M_3)\\) need not equal \\(P(D_1 \\cap \\{ D_1, D_2 \\}) = P(D_1)\\).",
    "crumbs": [
      "Probability and Inference",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History: Puzzles, Paradoxes, and Motivation</span>"
    ]
  },
  {
    "objectID": "1_theory/01_background_puzzles.html#puzzle-1",
    "href": "1_theory/01_background_puzzles.html#puzzle-1",
    "title": "History: Puzzles, Paradoxes, and Motivation",
    "section": "Puzzle #1",
    "text": "Puzzle #1\n\nThere are 43 cookies to be given out at random to 10 children. What is the probability that each child gets at least 2 cookies?\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\nExercise in flawed thinking.\n\nFind the flaw in the following logic:\nConsider the outcomes where each child has 2+ cookies. Then, for 20 cookies, we already know how those are distributed (each of the 10 children gets two cookies). The remaining 23 cookies can be assigned arbitrarily, so across the 10 kids there are a remaining \\(10^{23}\\) ways to distribute the cookies for each of the \\({43 \\choose 20}\\) ways that the 20-cookies already spoken-for can be distributed.\nThat gives us \\[\\frac{P(\\text{ways to get to every child having 2+ cookies})}{P(\\text{total number of ways to distribute 43 cookies})} =\n\\frac{{43 \\choose 20} \\cdot 10^{23}}{10^{43}} = \\frac{{43 \\choose 20}}{10^{20}}.\\]\n\n\nClick to reveal the problem in the above logic.\n\nI think we’re double-counting solutions when we say that for each of the \\({43 \\choose 20}\\) ways that we can say that the 20 cookies are already spoken for, there are another \\(10^{23}\\) ways to assign the other cookies.\nLet’s take a smaller example: assigning 4 cookies to 2 students, and I want to know how often to expect each student gets 1+ cookie. If I consider outcomes where cookies 1, 2, 3, 4 get assigned to children \\(1, 2, *, *\\), I would count \\(1,2,1,2\\) towards this tally. But if I later also consider outcomes of the form \\(*, *, 1, 2\\), I would again count the outcome \\(1,2,1,2\\) twice. This is wrong, and a crucial mistake.\nIn general, the real flaw was in not adequately formalizing our work and using heuristics like “if cookies are spoken for,” which is not a rigorously defined probability concept.\n\n\n\n\nFirst, notice that this is a classic multinomial setup. The multinomial distribution can be thought of as giving the probability of observing the outcome \\(i \\in \\{ 1, ..., k \\}\\) coming up \\(x_i\\) times when rolling a \\(k\\)-sided die \\(n\\) times, and is a generalization of the Binomial distribution. We have that the probability of each outcome \\(i\\) coming up on a single roll is given by \\(\\pi_i\\).\nIf the so-called die is fair, \\(\\pi_i = \\pi_{i'}\\; \\forall i, i' \\in \\{ 1, ..., k\\}\\) or the die is unfair and \\(\\pi_i\\) is not necessarily equal to \\(\\pi_i'\\). In all cases, we assume \\(\\sum \\pi_i = 1\\).\nLet \\(X = (X_1, ..., X_k)\\) be a \\(\\text{Multinomial}(n, k, \\pi)\\) distributed where \\(\\pi = (\\pi_1, ..., \\pi_k)\\).\nThe density of the Multinomial distribution is\n\\[\n\\begin{aligned}\nP(X = x) & = {n \\choose x_1,...,x_k!} \\prod_{i=1}^k \\pi_i^{x_i} \\\\\n& = \\frac{n!}{x_1!\\cdots x_k!}\\pi_1^{x_1} \\cdots \\pi_k^{x_k}.\n\\end{aligned}\n\\]\n\n\n\nIntuition\n\nI think of it this way: in the traditional Binomial outcome, there’s two outcomes that are being tallied, 1 or 0, and either 1 happens with probability \\(\\pi_1\\), or 0 happens with probability \\(1-\\pi_1\\) in each trial. Here, the \\(\\sum \\pi_i = 1\\) assumption is making it so that either outcome \\(i\\) happens with probability \\(\\pi_i\\), or one of the other \\(i' \\in \\{ 1, ..., k\\} \\backslash i\\) outcomes happens with probability \\(1 - \\pi_i\\).\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe Multinomial Theorem\n\nFor any positive integer \\(m\\) and non-negative integer \\(n\\), \\[(x_1 + ... + x_m)^n = \\sum_{k_1+...+ k_m = n} {n \\choose k_1, ..., k_m} \\prod_{i=1}^m x_i^{k_i},\\] where \\(k_1, ..., k_m \\in \\mathbb N\\) and \\({n \\choose k_1, ..., k_m} = \\frac{n!}{k_1!\\cdots k_m!}\\).\n\n\n\nReturning to the problem, let \\(p_n\\) denote the probability that all 10 children receive at least 2 cookies each given that \\(n\\) are given out uniformly at random.\nLet \\(X_1, ..., X_{10}\\) denote the random variables representing how many cookies each of the 10 children gets. Notice that \\[\n{\\small\nX_1, ..., X_{10} \\Bigg\\vert \\sum_{i=1}^{10} X_i = 43 \\sim\n\\text{Multinomial}(n = 43, k = 10, \\pi_i = 1/10),}\n\\] where \\(\\pi_i = 1/10\\) indicates that the distribution of cookies is uniformly random (e.g., equal probability) across the 10 children.\n\n\n\n\n\n\nPoisson-Multinomial Relationship\nThe Poisson and Multinomial distributions have an interesting relationship. When the outcomes \\(X_1, ..., X_k\\) are such that \\(X_i \\sim \\text{Poisson}(\\lambda_i)\\), then \\(\\sum_{i=1}^k X_i \\sim \\text{Poisson}(\\sum_{i=1}^k \\lambda_i).\\)\nOne can easily derive via the definition of conditional probability that \\[(X_1, ..., X_k) {\\Bigg\\vert} \\sum_{i=1}^k X_i = N = n \\sim \\text{Multinomial}(n, k, \\pi),\\] where \\(\\pi = \\left(\\frac{\\lambda_1}{\\sum \\lambda_i}, ..., \\frac{\\lambda_k}{\\sum \\lambda_i}\\right)\\).\nSee here for example.\n\n\\[\n\\begin{aligned}\nP(X = x \\Big \\vert N = n) & = \\frac{P(X = x, N = n)}{P(N = n)}  \\\\\n& = \\frac{P(X = x)}{P(N = n)} \\\\\n& = \\left( \\frac{e^{-\\sum \\lambda_i} \\prod \\lambda_i^{x_i}}{\\prod x_i!} \\right) \\Bigg / \\left( \\frac{e^{-\\sum \\lambda_i} (\\sum \\lambda_i)^{n}}{n!} \\right) \\\\\n& = {n \\choose x_1, ..., x_k} \\frac{\\prod \\lambda_i^{x_i}}{\\left( \\sum \\lambda_i \\right)^n} \\\\\n& = {n \\choose x_1, ..., x_k} \\left( \\frac{\\lambda_i}{\\sum \\lambda_i} \\right)^{x_i} \\\\\n& \\sim \\text{Multinomial}(n, k, \\pi).\n\\end{aligned}\n\\]\n\n\n\n\nSince no information was given to us about how it came about that \\(n = 43\\) cookies were given out, let’s assume that it was the result of a \\(\\text{Poisson}(\\lambda)\\) process. This implies that the \\(X_i\\) are independently and identically distributed as \\(\\text{Poisson}(\\lambda/10)\\) by a similar argument in the reverse direction.\n\n\\[P(X = x \\Bigg| \\sum X_i = n) = \\frac{P(X = x, \\sum X_i = n)}{\\underbrace{P(\\sum X_i = n)}_{\\text{Poisson}(\\lambda)}},\\] and \\(X = x \\implies \\sum X_i = \\sum x_i = n\\), so \\(P(X = x, \\sum X_i = n) = P(X = x)\\) as long as \\(n = \\sum x_i\\). \\[\n\\begin{aligned}\n\\therefore \\;\\; P(X = x) & = P(X = x \\Bigg| \\sum X_i = n) \\cdot P(\\sum X_i = n) \\\\\n& = \\left[ {n \\choose \\pi_1, ..., \\pi_k} \\prod \\pi_i^{x_i} \\right] \\cdot \\left( \\frac{e^{-\\lambda} \\lambda^{\\sum x_i}}{\\sum x_i! } \\right) \\\\\n& = \\frac{\\cancel{n!}}{x_1!\\cdots x_k!} \\pi_1^{x_1}\\cdots \\pi_k^{x_k} \\left( \\frac{e^{-\\lambda} \\lambda^{n}}{\\cancel{n!}} \\right).\n\\end{aligned}\n\\] Assume as given in the problem that the cookies are uniformly randomly given out, and \\(\\pi_i = \\pi_{i'}\\;  \\forall i, i' \\in \\{ 1, ..., k \\}\\); this single probability must be \\(1/k\\) (or in our case, \\(k = 10\\) children).\n\\[\n\\begin{aligned}\nP(X = x) & = \\frac{e^{-\\lambda}}{x_1! \\cdots x_k!} \\left( \\frac{\\lambda}{k} \\right)^{\\sum x_i} \\\\\n& = \\prod_{i=1}^k \\frac{e^{-\\lambda/k} \\left( \\frac{\\lambda}{k} \\right)^{x_i}}{x_i!}, \\\\\n\\text{a product of the }&\\text{density for $k$ iid Poisson}\\left(\\frac{\\lambda}{k}\\right) \\text{ variables}.\n\\end{aligned}\n\\]\n\nWe said that if \\(n\\) cookies are given out, then there’s a \\(p_n\\) probability that the 10 children all receive 2+ cookies. Then without conditioning on the number of cookies given out, the probability that all kids receive 2+ cookies is given by \\[\\E\\left[\\E[p_n | N = n]\\right] = \\sum_{n=0}^\\infty \\frac{e^{-\\lambda} \\lambda^n}{n!} p_n.\\]\nOn the other hand, since the \\(X_1, ..., X_k\\) are iid Poisson, the probability that all 10 kids receive 2+ cookies is \\(P(X_1 \\geq 2)^{10} = (1-P(X_1 \\leq 1))^{10} =\n(1-e^{-\\frac{\\lambda}{10}} - \\frac{\\lambda}{10}e^{-\\frac{\\lambda}{10}})^{10}\\).\nNow we have that \\[\n\\sum_{n=0}^\\infty \\frac{e^{-\\lambda} \\lambda^n}{n!} p_n\n=\n\\left[1-e^{-\\frac{\\lambda}{10}} - \\frac{\\lambda}{10}e^{-\\frac{\\lambda}{10}}\\right]^{10}.\n\\]\n\n\nReference materials on Poissonization and related ideas.\n\nSo, I didn’t know the first thing about “Poissonization,” but I’ve been trying to learn.\nIt appears that Mary Wootters [site] and C. Seshadhri [site] have nice online YouTube playlists of their randomized algorithms lectures in which they talked about Poissonization.\nSee the specific videos:\n\nhttps://www.youtube.com/watch?v=P71s05H2T5E\nhttps://www.youtube.com/watch?v=4lvfTKiLyXw\n\nI get the sense that the Chernoff bound for the Poisson-tail is something I should learn about further. The application of Stirling’s approximation to the distribution of deviations from the expected value of a Poisson process that Seshadhri presents is quite beautiful, and something I’d like to learn more about.\nThere’s also this interesting approximation of the Multinomial cell-specific count outcomes as Poisson that I saw popping up here and there (Ahmad 1985).\n\n\nAhmad, I. A. 1985. “On the Poisson Approximation of Multinomial Probabilities.” Statistics & Probability Letters 3 (1): 55–56. https://doi.org/10.1016/0167-7152(85)90013-6.\nNow, recall that \\(e^{x}\\) has a series expansion. If we multiply both sides by \\(e^{\\lambda}\\) and by \\(43!\\), we should find that the coefficient on the left-hand-side series for \\(\\lambda^{43}\\) is just \\(p_n\\), and the right-hand-side series coefficient for \\(\\lambda^{43}\\) gives an expression we can evaluate for \\(p_{43}\\). They must be equal, because in order for two convergent power series to coincide on a non-empty interval, their coefficients must be equal.\nSo what we’re going to do is expand the function \\(e^{x} = \\sum_{t=0}^\\infty \\frac{x^t}{t!}\\) wherever we see it on the modified right-hand-side and use that to identify the coefficient of \\(\\lambda^{43}\\), which is \\(p_{43}\\).\nHowever, this is a bit hard to do by hand, so we’ll use the symbolic algebra library sympy in Python to do it for us.\nfrom sympy import symbols, exp, factorial, series, Pow\n\nlambda_ = symbols('lambda')\n\ninner_expression = 1 - exp(-lambda_/10) - (lambda_/10)*exp(-lambda_/10)\n\nraised_expression = inner_expression**10 \n\ncomplete_expression = exp(lambda_) * raised_expression\n\nexpanded_series = series(complete_expression, x=lambda_, n=44).removeO()\n\ncoeff_lambda_43 = expanded_series.coeff(lambda_**43)\n\np_43 = factorial(43) * coeff_lambda_43\np_43\n\\[\n\\frac{38360235213946776318553037176114920309}{78125000000000000000000000000000000000} \\approx 0.491\n\\]\nThus we conclude that if 43 cookies are given out to 10 children uniformly at random, then the probability that each child receives at least 2 cookies is \\(\\approx .491.\\)\nLet’s see if we can confirm that via a simple simulation in R:\nset.seed(1234)\n\nnum_trials &lt;- 100000  # Number of simulations\nnum_children &lt;- 10    # Number of children\nnum_cookies &lt;- 43     # Number of cookies\n\nresults &lt;- replicate(num_trials, {\n  cookies &lt;- sample(1:num_children, num_cookies, replace = TRUE)\n  counts &lt;- table(factor(cookies, levels = 1:10))\n  all(counts &gt;= 2)\n})\n\nprob_estimate &lt;- mean(results)\nvar_estimate &lt;- var(results) / num_trials\n\nprob_estimate\nvar_estimate\n&gt; prob_estimate\n[1] 0.49178\n&gt; var_estimate\n[1] 2.499349e-06",
    "crumbs": [
      "Probability and Inference",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History: Puzzles, Paradoxes, and Motivation</span>"
    ]
  },
  {
    "objectID": "3_datastructures/01_random_numbers.html",
    "href": "3_datastructures/01_random_numbers.html",
    "title": "Random Numbers",
    "section": "",
    "text": "There are plenty of suboptimal ways to produce pseudorandom numbers. Some that we looked at are the middle-square method and linear congruential generators.\n\n\n\n\n\nJohn von Neumann (1903-1957) was a Hungarian and American mathematician, physicist, computer scientist, engineer and polymath, to quote Wikipedia.\n\n\nA crucial flaw with the mid-square method was that if a prior state had two zeroes behind the decimal point, they never disappear, leading to predictability in the not-very-random pseudorandom numbers generated.\nA challenging flaw of the linear congruential generator approach is that the generated pseudorandom numbers live on a relatively small number of hyperplanes (at most \\(\\sqrt[n]{n! \\cdot m}\\) due to Marsaglia’s theorem). The exact number of hyperparameters can be adjusted by selecting \\(n\\) and \\(m\\), but if carelessly chosen, this can represent a weakness of the pseudorandom numbers.\n\n\n\n\nFrom the LCG Wikipedia Page\n\n\n\n\nDesiderata of Pseudoranom Numbers:\n\nUniformity on [0,1]\nIndependence\nDiehard tests\nReplication\nCycle Length\nSpeed\nMemory usage\nParallel Implementation\nCryptographically secure\n\n\nRelated topics:\n\nHashing\nPassword Storage and Verification\nVerification of Successful Data Transfer",
    "crumbs": [
      "Data Structures & Algorithms",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "3_datastructures/01_random_numbers.html#related-topics",
    "href": "3_datastructures/01_random_numbers.html#related-topics",
    "title": "Random Numbers",
    "section": "Related topics:",
    "text": "Related topics:\n\nHashing\nPassword Storage and Verification\nVerification of Successful Data Transfer",
    "crumbs": [
      "Data Structures & Algorithms",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "3_datastructures/01_random_numbers.html#brief-intro-to-hashing",
    "href": "3_datastructures/01_random_numbers.html#brief-intro-to-hashing",
    "title": "Random Numbers",
    "section": "Brief intro to Hashing",
    "text": "Brief intro to Hashing\nReferences:\n\nhttps://www.geeksforgeeks.org/what-is-hashing/\nhttps://en.wikipedia.org/wiki/Hash_function\n\nFrom https://web.stanford.edu/class/archive/cs/cs106b/cs106b.1136/lectures/20/Slides20.pdf:\nA hash function converts a large object (a genome, a string, a sequence, etc) into a smaller object (a shorter string, an integer, etc).\nA hash function must necessarily be deterministic: given the same input, it must always produce the same output. (Why? Because we want to use it for comparison or lookup. It needs to be a reliable shortcut.)\nA hash function should try to produce different outputs for different inputs. I.e., we want to minimize hash collisions.\nWhy are hash functions related to pseudorandomness? Because a good hash function will scramble up the bits of the input into something that looks random.",
    "crumbs": [
      "Data Structures & Algorithms",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "3_datastructures/01_random_numbers.html#desiderata-of-pseudoranom-numbers",
    "href": "3_datastructures/01_random_numbers.html#desiderata-of-pseudoranom-numbers",
    "title": "Random Numbers",
    "section": "Desiderata of Pseudoranom Numbers:",
    "text": "Desiderata of Pseudoranom Numbers:\n\nUniformity on [0,1]\nIndependence\nDiehard tests\nReplication\nCycle Length\nSpeed\nMemory usage\nParallel Implementation\nCryptographically secure",
    "crumbs": [
      "Data Structures & Algorithms",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "3_datastructures/01_random_numbers.html#password-verification",
    "href": "3_datastructures/01_random_numbers.html#password-verification",
    "title": "Random Numbers",
    "section": "Password Verification",
    "text": "Password Verification\nWe can use cryptographic hashes to store merely a hash of the user’s password rather than their actual password, thereby enhancing security.\nSee https://en.wikipedia.org/wiki/Cryptographic_hash_function#Password_verification\nIntriguingly, Wikipedia says “use of standard cryptographic hash functions, such as the SHA series, is no longer considered safe for password storage.”",
    "crumbs": [
      "Data Structures & Algorithms",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "3_datastructures/01_random_numbers.html#verification-of-data-transfer",
    "href": "3_datastructures/01_random_numbers.html#verification-of-data-transfer",
    "title": "Random Numbers",
    "section": "Verification of Data Transfer",
    "text": "Verification of Data Transfer\nSee https://en.wikipedia.org/wiki/Cryptographic_hash_function#Verifying_the_integrity_of_messages_and_files\n“An important application of secure hashes is the verification of message integrity.”",
    "crumbs": [
      "Data Structures & Algorithms",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "3_datastructures/01_random_numbers.html#what-makes-a-hash-cryptographically-secure",
    "href": "3_datastructures/01_random_numbers.html#what-makes-a-hash-cryptographically-secure",
    "title": "Random Numbers",
    "section": "What makes a hash cryptographically secure?",
    "text": "What makes a hash cryptographically secure?\nWell, we can define it by its converse: a cryptographic hash should be resistant against a cryptanalytic attack, or in other words:\n\n(Preimage resistance) It should be hard to recover the pre-image m from only having h = hash(m)\n(Weak Collision Resistance) Given m1, it should be difficult to find another m2 such that hash(m1) = hash(m2)\n(Strong Collision Resistance) It should be difficult to find any pair m1, m2 such that hash(m1) = hash(m2)\n\n\nThis YouTube video looks quite nice — enjoy if you have time:",
    "crumbs": [
      "Data Structures & Algorithms",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "2_methods/01_linear_regression.html",
    "href": "2_methods/01_linear_regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "On the history of regression:\nSee Stigler (1981) on the invention of least squares, and Stigler (1997) discussing the history of linear “regression”.",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "2_methods/01_linear_regression.html#visualizing-interactions",
    "href": "2_methods/01_linear_regression.html#visualizing-interactions",
    "title": "Linear Regression",
    "section": "Visualizing Interactions",
    "text": "Visualizing Interactions\nOne of the interesting things from Methods was learning about useful off-the-shelf tools like jtools::summ and interactions::interact_plot.\nHere, I’ll construct an example to show off interactions::interact_plot based on a hypothetical research question focused on how the harmful effects of structural racism compound over the lifecourse, so we suspect there should be a (potentially nonlinear) interaction between age and race on health measures when we lack any observations of explicit measures of discrimination to study. Below are models fit adjusting for age-race/ethnicity interaction and then we use interactions::interact_plot to visualize the difference in average systolic blood pressure.\nThese examples are for educational purposes only, and not scientific findings.\nThe dataset I’ll use is available from the NHANES package. Check out ?NHANES::NHANES for more details, but some useful excerpts:\n\nThe NHANES target population is “the non-institutionalized civilian resident population of the United States”. NHANES, (American National Health and Nutrition Examination surveys), use complex survey designs (see http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf) that oversample certain subpopulations like racial minorities. Naive analysis of the original NHANES data can lead to mistaken conclusions. The percentages of people from each racial group in the data, for example, are quite different from the way they are in the population.\n\n\n… NHANES contains 10,000 rows of data resampled from NHANESraw to undo these oversampling effects. NHANES can be treated, for educational purposes, as if it were a simple random sample from the American population.\n\n\nlibrary(NHANES)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(mgcv)\nlibrary(patchwork)\nlibrary(magrittr)\n\ndf &lt;- NHANES::NHANES \n\n# It's important to pay attention to reference categories. Here, our categorical\n# variables are Race, Education, and Marital Status. \ndf %&lt;&gt;% mutate(\n  Race1 = relevel(Race1, 'White'),\n  Education = relevel(Education, 'College Grad'),\n  MaritalStatus = relevel(MaritalStatus, \"Married\")\n  )\n\nlm_model &lt;- lm(\n  BPSysAve ~ AgeMonths * Race1 + Education + MaritalStatus + Poverty,\n  data = df\n)\n\ngam_model &lt;- gam(\n  BPSysAve ~ s(AgeMonths, by = Race1) + Education + MaritalStatus + s(Poverty),\n  data = df\n)\n\nlm_plt &lt;- interactions::interact_plot(\n  lm_model, pred = AgeMonths, modx = Race1, interval = TRUE)\n\ngam_plt &lt;- interactions::interact_plot(\n  gam_model, pred = AgeMonths, modx = Race1, interval = TRUE)\n\nlm_plt &lt;- lm_plt + \n  ggtitle(\"Race-Age Interaction Effects on Sys. Blood Pressure\",\n  subtitle = 'Linear Model') + \n  guides(\n    linetype = guide_legend(nrow = 2, byrow = TRUE), \n    fill = guide_legend(nrow = 2, byrow = TRUE), \n    color = guide_legend(nrow = 2, byrow = TRUE)) +\n  theme(legend.position = 'bottom')\n\ngam_plt &lt;- gam_plt + \n  ggtitle(\"Race-Age Interaction Effects on Sys. Blood Pressure\",\n  subtitle = 'Generalized Additive Model') + \n  theme(legend.position = 'bottom') + \n  guides(\n    linetype = guide_legend(nrow = 2, byrow = TRUE), \n    fill = guide_legend(nrow = 2, byrow = TRUE), \n    color=guide_legend(nrow=2,byrow=TRUE)\n    ) \n\nlm_plt + gam_plt\n\n\n\n\n\n\n\n\n\n\nWhy is the Race1 variable like that?\n\nI find the levels of what NHANES provides as the Race1 variable kind of interesting in that Mexican and Hispanic are distinguished, so I wanted to check on that a bit further:\n\n# checking on racial categorization\ntable(NHANES::NHANES$Race1)\n\n\n   Black Hispanic  Mexican    White    Other \n    1197      610     1015     6372      806 \n\n\nI’m not entirely sure why Hispanic / Mexican are distinguished, but one might find explanation in this handy Pew Research chart on “What Census Calls Us: A Historical Timeline”: https://www.pewresearch.org/wp-content/uploads/2020/02/PH_15.06.11_MultiRacial-Timeline.pdf\nThe NHANES data are from the 2009-2012 sample years, so one can see that the Census distinguishes Hispanic Ethnicity into\n\nMexican, Mexican American, Chicano\nPuerto Rican\nCuban\nAnother Hispanic, Latino, Spanish origin\n\nI wonder if the CDC NHANES was trying to capture “Race-Ethnicity” in one variable and have stratified ethnicity into just “Mexican” vs. “All Other Hispanic”.\n\nWe can interpret the above figure as showing elevated systolic blood pressure associated with older age, especially among the Hispanic population after adjusting for differences associated with education, marital status, and the ratio of household income to the poverty-line.",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "2_methods/01_linear_regression.html#linear-regression",
    "href": "2_methods/01_linear_regression.html#linear-regression",
    "title": "Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nOur typical problem is to estimate the linear relationship between \\(Y\\) and \\(p\\) covariates/predictors \\((x_1, ..., x_p)\\).\nIn modeling, we can distinguish between the systematic and random parts of a model.\nConsider the model:\n\\[Y_i = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_{ip}, \\quad i = 1, ..., n,\\]\nwith the assumptions:\n\n(Size of Data) \\(p &lt; n\\),\n(Mean-Zero Error) \\(\\E(\\varepsilon_i) = 0\\)\n(Homoscedasticity) \\(\\Var(\\varepsilon_i) = \\sigma^2\\),\n(Uncorrelated Error) \\(\\Cov(\\varepsilon_i, \\varepsilon_j) = 0.\\)\n\nWe can also write this model in the following way: \\[\\mathbf Y = \\mathbf X \\pmb \\beta + \\pmb \\epsilon, \\quad \\text{ where } \\] \\[\\mathbf Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}, \\quad\n\\pmb X = \\begin{bmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix},\\] \\[\\pmb \\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}, \\quad\n\\pmb \\epsilon = \\begin{bmatrix} \\epsilon_0 \\\\ \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_p\\end{bmatrix}.\\]\nWe assume that this is the true model. Our task is to do the best we can coming up with \\(\\hat {\\pmb \\beta}\\) such that \\(\\mathbf X \\hat{\\pmb \\beta}\\) is as close as possible to \\(\\mathbf y\\).\nWe’ll use \\(\\mathbf y\\) to refer to observed values and \\(\\mathbf Y\\) to denote the corresponding random vector that generates these observations. That is to say, \\(\\E(\\mathbf Y) = \\mathbf X \\pmb \\beta\\).\nEuclidean distance is usually defined as \\[d(\\mathbf y, \\mathbf X \\pmb \\beta) = \\sqrt{(\\mathbf y - \\mathbf X \\pmb \\beta)^\\top (\\mathbf y - \\mathbf X \\pmb \\beta)}.\\]\nSince it’s easier to minimize over a function that doesn’t involve a square-root, and squaring is a monotone transformation on \\(\\mathbb R^+\\), the vector \\(\\hat{\\pmb \\beta}\\) that minimizes \\(d(\\mathbf y, \\mathbf X \\pmb \\beta)\\) will also minimize \\(d(\\mathbf y, \\mathbf X \\pmb \\beta)^2\\).\n\\[\n\\begin{aligned}\n\\text{Let } S(\\pmb \\beta) & =d(\\mathbf y, \\mathbf X \\pmb \\beta)^2 = (\\mathbf y - \\mathbf X \\pmb \\beta)^\\top (\\mathbf y - \\mathbf X \\pmb \\beta)  \\\\\n& = \\mathbf y^\\top \\mathbf y - 2 \\mathbf y^\\top \\mathbf X \\pmb \\beta + \\pmb \\beta^\\top \\mathbf X^\\top \\mathbf X \\pmb \\beta.\n\\end{aligned}\n\\]\nWhy do we call this \\(S\\)?\n\nIt is the squared Euclidean distance;\nAlso the sum of squared errors/residuals (SSE).\n\n\\(S(\\pmb \\beta)\\) is our objective function, and the minimization of \\(S(\\pmb \\beta)\\) under the assumption of normal errors leads to the best linear unbiased estimator (BLUE) of the regression coefficients due to the Gauss-Markov Theorem.\nTo find the OLS estimate, we minimize our loss function by computing the gradient, setting it equal to zero, and solving for the coefficient vector \\(\\pmb \\beta\\) satisfying this constraint.\n\\[\n\\begin{aligned}\n\\pp[S(\\pmb \\beta)]{\\pmb \\beta} & = -2 \\mathbf X^\\top \\mathbf y + 2 \\mathbf X^\\top\\mathbf X \\pmb \\beta \\stackrel {set} = \\pmb 0 \\\\\n& 2 \\mathbf X^\\top (\\mathbf y - \\mathbf X\\pmb \\beta) = 0\n\\end{aligned}\n\\] From this we get the least squares normal equations: \\[\\mathbf X^\\top \\mathbf X \\hat {\\pmb \\beta} = \\mathbf X'\\mathbf y,\\] \\[ \\hat \\beta = (\\mathbf X^{\\top} \\mathbf X)^{-1} \\mathbf X^\\top \\mathbf y,\\] provided that \\((\\mathbf X^\\top \\mathbf X)^{-1}\\) exists, which it will if the predictors are linearly independent.\n\n\n\n\n\n\nTip\n\n\n\n\n\nHow exactly was the above worked out?\n\nFirst, expand \\(S(\\pmb \\beta)\\): \\[\n\\begin{aligned}\nS(\\pmb \\beta) & = (\\mathbf y - \\mathbf X \\pmb \\beta)^{\\top}(\\mathbf y - \\mathbf X \\pmb \\beta)\\\\\n& = \\mathbf y^\\top \\mathbf y - \\mathbf y^\\top \\mathbf X \\pmb \\beta - (\\mathbf X \\pmb \\beta)^{\\top} \\mathbf y + (\\mathbf X \\pmb \\beta)^\\top \\mathbf X \\pmb \\beta\n\\end{aligned}\n\\]\nSince \\((\\mathbf X \\pmb \\beta)^\\top \\mathbf y = \\mathbf y^\\top \\mathbf X \\pmb \\beta\\) (as it’s a scalar and equal to its transpose), we can simplify to \\[\nS(\\pmb \\beta) = \\mathbf y^\\top \\mathbf y - 2 \\mathbf y^\\top \\mathbf X \\pmb \\beta + \\pmb \\beta^\\top \\mathbf X^\\top \\mathbf X \\pmb \\beta.\n\\]\n\nThe derivative of \\(\\mathbf y^\\top \\mathbf y\\) with respect to \\(\\pmb \\beta\\) is 0\nThe derivative of \\(-2\\mathbf y^\\top \\mathbf X \\pmb \\beta\\) with respect to \\(\\pmb \\beta\\) is \\(-2 \\mathbf X^\\top \\mathbf y\\) since the derivative of \\(\\mathbf a^\\top \\pmb \\beta\\) is \\(\\mathbf a\\). (Page 10, 2.4.1 of Petersen and Pedersen (2012))\nThe derivative of \\(\\pmb \\beta^\\top \\mathbf X^\\top \\mathbf X \\pmb \\beta\\) with respect to \\(\\pmb \\beta\\) is \\(2 \\mathbf X^\\top \\mathbf X \\pmb \\beta\\) by applying the derivative rule for quadratic forms. (See equation 81 from Petersen and Pedersen (2012), and observe that \\(\\mathbf X^\\top \\mathbf X\\) is symmetric).\n\nHence \\[\\nabla S(\\pmb \\beta) = -2 \\mathbf X^\\top \\mathbf y + 2 \\mathbf X^\\top \\mathbf X \\pmb \\beta = \\pmb 0.\n\\]\nSetting \\(\\nabla S(\\pmb \\beta) = 0\\) gives us the normal equations: \\[\\mathbf X^\\top \\mathbf X \\hat {\\pmb \\beta} = \\mathbf X^\\top \\mathbf y.\\]\nFinally we solve for \\(\\hat \\beta\\) by multiplying both sides by \\((\\mathbf X^\\top \\mathbf X)^{-1}\\): \\[\\hat \\beta = (\\mathbf X^\\top \\mathbf X)^{-1} \\mathbf X^\\top \\mathbf y.\\]\n\n\n\n\nPetersen, Kaare Brandt, and Michael Syskind Pedersen. 2012. “The Matrix Cookbook.” https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf.",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  }
]